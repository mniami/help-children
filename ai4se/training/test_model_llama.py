"""
Test Llama-3.2-1B fine-tuned medical model
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def test_model(model_path, base_model_name):
    """Test the fine-tuned model"""
    
    print("=" * 70)
    print("Testing Llama-3.2-1B Medical Model")
    print("Optimized for: MEIZU Mblu 21 (4GB RAM)")
    print("=" * 70)
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Load base model
    print("\nLoading base model (1.23B parameters)...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    
    # Load LoRA adapter
    print("Loading fine-tuned medical adapter...")
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    print("‚úÖ Model loaded successfully!\n")
    
    # Test cases
    test_prompts = [
        {
            "name": "Child with fever and diarrhea",
            "messages": [
                {"role": "system", "content": "You are a medical assistant for primary health diagnosis in rural areas. Your role is to ask relevant questions about symptoms, provide initial assessment, suggest immediate actions, and identify when professional medical care is urgently needed."},
                {"role": "user", "content": "My child has fever and diarrhea for 3 days. What should I do?"}
            ]
        },
        {
            "name": "Chest pain emergency",
            "messages": [
                {"role": "system", "content": "You are a medical assistant for primary health diagnosis in rural areas. Your role is to ask relevant questions about symptoms, provide initial assessment, suggest immediate actions, and identify when professional medical care is urgently needed."},
                {"role": "user", "content": "I have chest pain and difficulty breathing. What's happening?"}
            ]
        },
        {
            "name": "Baby feeding concerns",
            "messages": [
                {"role": "system", "content": "You are a medical assistant for primary health diagnosis in rural areas. Your role is to ask relevant questions about symptoms, provide initial assessment, suggest immediate actions, and identify when professional medical care is urgently needed."},
                {"role": "user", "content": "My 6-month-old baby is not eating well for 2 days. Should I be worried?"}
            ]
        }
    ]
    
    for i, test_case in enumerate(test_prompts, 1):
        print("\n" + "=" * 70)
        print(f"Test Case {i}: {test_case['name']}")
        print("=" * 70)
        
        # Format using Llama 3 chat template
        messages = test_case['messages']
        prompt = "<|begin_of_text|>"
        
        for msg in messages:
            role = msg['role']
            content = msg['content']
            if role == "system":
                prompt += f"<|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>"
            elif role == "user":
                prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>"
        
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        
        print(f"\nQuestion: {test_case['messages'][-1]['content']}\n")
        print("Response:")
        print("-" * 70)
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract just the assistant's response
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            assistant_response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
            assistant_response = assistant_response.replace("<|eot_id|>", "").strip()
            print(assistant_response)
        else:
            print(response)
        
        print("-" * 70)
    
    print("\n" + "=" * 70)
    print("‚úÖ Testing complete!")
    print("=" * 70)
    print("\nüì± Model Performance Estimate on MEIZU Mblu 21:")
    print("   - Model size (merged): ~2.5GB unquantized")
    print("   - Model size (Q4): ~600MB")
    print("   - RAM usage: ~1.5GB")
    print("   - Speed: 4-8 tokens/second")
    print("   - Quality: ‚≠ê‚≠ê‚≠ê‚≠ê High accuracy")
    print()


if __name__ == "__main__":
    model_path = "models/llama-1b-medical-lora"
    base_model_name = "meta-llama/Llama-3.2-1B-Instruct"
    
    test_model(model_path, base_model_name)
