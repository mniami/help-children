"""
Amharic Medical Voice Assistant for Mobile (MEIZU Mblu 21 Optimized)

Complete voice pipeline optimized for 4GB RAM devices:
- Whisper-small for Amharic Speech-to-Text
- Fine-tuned medical model for diagnosis
- Simple audio playback for responses

Optimizations:
- Lightweight models (fits in 4GB RAM)
- CPU-optimized inference
- Streaming responses
- Battery efficient
"""

import whisper
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import soundfile as sf
import numpy as np
import os
from datetime import datetime


class AmharicMedicalAssistant:
    """
    Amharic-enabled medical assistant optimized for mobile devices
    """

    def __init__(
        self,
        base_model_name: str = "HuggingFaceTB/SmolLM2-135M-Instruct",
        lora_adapter_path: str = "models/medical-lora-test",
        whisper_model: str = "small",  # base for mobile, small for better accuracy
        device: str = "cpu"
    ):
        """
        Initialize Amharic Medical Assistant

        Args:
            base_model_name: Base model name
            lora_adapter_path: Path to fine-tuned medical LoRA adapter
            whisper_model: Whisper model size ('tiny', 'base', 'small')
            device: Device to use (default: 'cpu' for mobile)
        """
        self.device = device
        self.conversation_history = []
        
        print("=" * 60)
        print("Amharic Medical Assistant - Loading Models")
        print("=" * 60)
        
        self._load_models(base_model_name, lora_adapter_path, whisper_model)
        self._setup_system_prompt()

    def _load_models(self, base_model_name, lora_adapter_path, whisper_size):
        """Load all required models"""

        # 1. Load Speech-to-Text (Whisper for Amharic)
        print(f"\n1Ô∏è‚É£ Loading Whisper-{whisper_size} for Amharic STT...")
        print(f"   Size: ~{self._get_whisper_size(whisper_size)}")
        print(f"   Languages: 99 including Amharic (·ä†·àõ·à≠·äõ)")
        
        self.stt_model = whisper.load_model(whisper_size, device=self.device)
        print("   ‚úÖ Whisper loaded")

        # 2. Load Fine-tuned Medical Model
        print(f"\n2Ô∏è‚É£ Loading medical model...")
        print(f"   Base: {base_model_name}")
        print(f"   Medical LoRA: {lora_adapter_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,  # FP32 for CPU
            trust_remote_code=True
        )
        
        # Load LoRA adapter if path exists
        if os.path.exists(lora_adapter_path):
            self.model = PeftModel.from_pretrained(base_model, lora_adapter_path)
            print(f"   ‚úÖ Medical LoRA adapter loaded")
        else:
            self.model = base_model
            print(f"   ‚ö†Ô∏è  Using base model (no adapter found)")
        
        self.model.eval()
        print("   ‚úÖ Medical model loaded")

        # 3. TTS Setup (Optional - for future)
        print(f"\n3Ô∏è‚É£ Text-to-Speech:")
        print(f"   Status: Not loaded (to save RAM)")
        print(f"   Note: Can add Piper TTS for Amharic voice output")
        self.tts_enabled = False

    def _get_whisper_size(self, model_size):
        """Get approximate model size"""
        sizes = {
            "tiny": "75 MB",
            "base": "140 MB",
            "small": "460 MB",
            "medium": "1.5 GB"
        }
        return sizes.get(model_size, "Unknown")

    def _setup_system_prompt(self):
        """Setup bilingual system prompt (Amharic + English)"""
        self.system_prompt = """·ä†·äï·â∞ ·ã®·å§·äì ·ä†·àõ·ä´·à™ ·äê·àÖ·ç¢ You are a medical assistant for primary health diagnosis.

Your role / ·àö·äì:
1. Listen to patient symptoms / ·àù·àç·ä≠·â∂·âΩ·äï ·àõ·ã≥·àò·å•
2. Ask relevant questions / ·å•·ã´·âÑ·ãé·âΩ·äï ·àò·å†·ã®·âÖ
3. Provide initial assessment / ·ã®·àò·åÄ·àò·à™·ã´ ·àù·ãò·äì ·àò·àµ·å†·âµ
4. Suggest immediate actions / ·ãà·ã≤·ã´·ãç·äë ·ä•·à≠·àù·åÉ·ãé·âΩ·äï ·àõ·àµ·â∞·àã·àà·çç
5. Identify emergencies / ·ä†·àµ·â∏·ä≥·ã≠ ·àÅ·äî·â≥·ãé·âΩ·äï ·àò·àà·ã®·âµ

IMPORTANT:
- Always be clear and compassionate
- Ask one question at a time
- Use simple language
- Identify when urgent care is needed
- Respond in both Amharic and English when helpful

·â∞·å®·àõ·à™ ·àò·à®·åÉ ·ä´·àµ·çà·àà·åà ·å•·ã´·âÑ·ãé·âΩ·äï ·ã≠·å†·ã≠·âÅ·ç¢"""

    def transcribe_audio(self, audio_path: str) -> dict:
        """
        Transcribe Amharic speech to text

        Args:
            audio_path: Path to audio file (WAV, MP3, etc.)

        Returns:
            dict with 'text', 'language', 'confidence'
        """
        print(f"\nüé§ Transcribing audio: {audio_path}")
        
        result = self.stt_model.transcribe(
            audio_path,
            language="am",  # Amharic
            task="transcribe",
            fp16=False,  # Use FP32 for CPU
            verbose=False
        )

        transcribed_text = result["text"].strip()
        
        # Detect if response contains Amharic characters
        has_amharic = any('\u1200' <= char <= '\u137F' for char in transcribed_text)
        
        print(f"   Text: {transcribed_text}")
        print(f"   Language: {'Amharic (·ä†·àõ·à≠·äõ)' if has_amharic else 'English'}")
        print(f"   ‚úÖ Transcription complete")
        
        return {
            "text": transcribed_text,
            "language": "amharic" if has_amharic else "english",
            "confidence": "high"  # Whisper doesn't provide confidence scores
        }

    def generate_response(self, user_message: str) -> str:
        """
        Generate medical advice based on user message

        Args:
            user_message: Patient's message (Amharic or English)

        Returns:
            Medical assistant's response
        """
        print(f"\nü§ñ Generating medical response...")
        
        # Build prompt with system instructions and conversation
        prompt = f"System: {self.system_prompt}\n\n"
        
        # Add conversation history
        for msg in self.conversation_history[-4:]:  # Last 4 messages for context
            prompt += f"{msg['role']}: {msg['content']}\n\n"
        
        # Add current message
        prompt += f"User: {user_message}\n\nAssistant:"
        
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # Generate response
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the assistant's response
        if "Assistant:" in response:
            response = response.split("Assistant:")[-1].strip()
        
        print(f"   Response length: {len(response)} characters")
        print(f"   ‚úÖ Response generated")
        
        # Update conversation history
        self.conversation_history.append({"role": "User", "content": user_message})
        self.conversation_history.append({"role": "Assistant", "content": response})
        
        return response

    def process_voice_input(self, audio_path: str) -> dict:
        """
        Complete voice pipeline: Audio ‚Üí Text ‚Üí Response

        Args:
            audio_path: Path to audio file

        Returns:
            dict with 'transcribed_text', 'response', 'timestamp'
        """
        print("\n" + "=" * 60)
        print("Processing Voice Input")
        print("=" * 60)
        
        # Step 1: Transcribe audio (Amharic ‚Üí Text)
        transcription = self.transcribe_audio(audio_path)
        transcribed_text = transcription["text"]
        
        # Step 2: Generate medical response
        response = self.generate_response(transcribed_text)
        
        print("\n" + "=" * 60)
        print("Voice Processing Complete")
        print("=" * 60)
        
        return {
            "transcribed_text": transcribed_text,
            "detected_language": transcription["language"],
            "response": response,
            "timestamp": datetime.now().isoformat()
        }

    def chat(self, message: str) -> str:
        """
        Simple text-based chat (no voice)

        Args:
            message: User's text message

        Returns:
            Assistant's response
        """
        return self.generate_response(message)

    def reset_conversation(self):
        """Clear conversation history"""
        self.conversation_history = []
        print("Conversation history cleared")

    def save_conversation(self, filepath: str):
        """Save conversation to file"""
        import json
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        print(f"Conversation saved to: {filepath}")


def demo_text_mode():
    """Demo: Text-based medical consultation"""
    print("\n" + "=" * 60)
    print("DEMO: Text-Based Medical Consultation")
    print("=" * 60 + "\n")
    
    assistant = AmharicMedicalAssistant()
    
    print("\n" + "=" * 60)
    print("Ready for consultation!")
    print("=" * 60 + "\n")
    
    # Test cases
    test_messages = [
        "·àç·åÑ ·àà3 ·âÄ·äì·âµ ·âµ·ä©·à≥·âµ ·ä•·äì ·â∞·âÖ·àõ·âµ ·ä†·àà·â†·âµ·ç¢ ·àù·äï ·àõ·ãµ·à®·åç ·ä†·àà·â•·äù?",  # Amharic
        "My child has fever and diarrhea for 3 days. What should I do?",  # English
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\n{'='*60}")
        print(f"Test Case {i}")
        print(f"{'='*60}")
        print(f"User: {message}\n")
        
        response = assistant.chat(message)
        print(f"Assistant: {response}\n")


def demo_voice_mode():
    """Demo: Voice-based medical consultation (requires audio file)"""
    print("\n" + "=" * 60)
    print("DEMO: Voice-Based Medical Consultation")
    print("=" * 60 + "\n")
    
    assistant = AmharicMedicalAssistant()
    
    # Check for sample audio
    sample_audio = "sample_amharic.wav"
    
    if os.path.exists(sample_audio):
        result = assistant.process_voice_input(sample_audio)
        
        print(f"\nüìù Transcription: {result['transcribed_text']}")
        print(f"üåê Language: {result['detected_language']}")
        print(f"üí¨ Response: {result['response']}")
    else:
        print(f"‚ö†Ô∏è  No audio file found: {sample_audio}")
        print(f"   To test voice mode:")
        print(f"   1. Record Amharic audio (WAV or MP3)")
        print(f"   2. Save as '{sample_audio}'")
        print(f"   3. Run this demo again")


if __name__ == "__main__":
    print("=" * 60)
    print("Amharic Medical Voice Assistant")
    print("Optimized for MEIZU Mblu 21 (4GB RAM)")
    print("=" * 60)
    
    # Run text demo
    demo_text_mode()
    
    # Uncomment to test voice mode:
    # demo_voice_mode()
